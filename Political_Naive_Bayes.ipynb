{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishokn/ADS-509---Module-4/blob/main/Political_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_5XWAmHs42y"
      },
      "source": [
        "# Naive Bayes on Political Text\n",
        "\n",
        "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Feel free to include your text patterns functions\n",
        "#from text_functions_solutions import clean_tokenize, get_patterns\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B63TzOjh8IXR",
        "outputId": "dd4df5e8-a795-4cf6-8c82-e48378977b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xptozvTks420"
      },
      "outputs": [],
      "source": [
        "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
        "convention_cur = convention_db.cursor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nOJ0J8rs420"
      },
      "source": [
        "## 1. Exploratory Naive Bayes\n",
        "\n",
        "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" exercise. First, we'll pull in the text\n",
        "for each party and prepare it for use in Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convention_data = []\n",
        "# fill this list up with items that are themselves lists. The\n",
        "# sublists will have two elements. The first element in the sublist\n",
        "# should be a string of the cleaned words in the speech. The second\n",
        "# element should be the party.\n",
        "\n",
        "# Define the cleaning and tokenizing function\n",
        "def clean_tokenize(text):\n",
        "    # Convert text to lowercase and tokenize\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Keep only alphabetic tokens\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    # Return as a single string of tokens\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "query_results = convention_cur.execute(\n",
        "    '''\n",
        "    SELECT text, party\n",
        "    FROM conventions\n",
        "    '''\n",
        ")\n",
        "\n",
        "for row in query_results :\n",
        "    text = row[0]  # the raw text\n",
        "    party = row[1]  # the associated party\n",
        "    cleaned_text = clean_tokenize(text)\n",
        "    convention_data.append([cleaned_text, party])"
      ],
      "metadata": {
        "id": "a-qoy5vb78d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH_I61xis420"
      },
      "outputs": [],
      "source": [
        "# it's a best practice to close up your DB connection when you're done\n",
        "convention_db.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjNJoiqds420"
      },
      "source": [
        "Let's look at some random entries and see if they look right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG_nGrbws421",
        "outputId": "3bc43ed1-f309-4c40-ad76-3dfc5d208798"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['and the benefits of that response extend far beyond coronavirus will continue to aid many that are just unable to find transportation or way to the doctor for regular checkups this is especially true in rural america i live in a town of about people we do not have buses trains trolleys or uber s available to us in addition the unavailability of services can also hinder treatment for many so increased access to from millions of americans has truly been and we have president trump to thank',\n",
              "  'Republican'],\n",
              " ['that s good well say hello to the folks in utah because they are great people thank you very much congratulations',\n",
              "  'Republican'],\n",
              " ['and then he comes back to me then he goes it was lovely talking to your and then he starts the interview with dana bash it was unbelievable',\n",
              "  'Democratic'],\n",
              " ['what she says for the people inaudible every ounce of who she is she is for us she s for us',\n",
              "  'Democratic'],\n",
              " ['gino is a truck driver from ohio who heard politicians for years make empty promises about defending american jobs only to see those promises broken again and again but in when general motors closed its plant in lordstown ohio president donald trump refused to stand by and watch it happen and as gino observed this president reached out to general motors to find a way to bring jobs back to lordstown and plans were soon set into motion to create lordstown motors',\n",
              "  'Republican']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "random.choices(convention_data,k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo2aarTzs421"
      },
      "source": [
        "It'll be useful for us to have a large sample size than 2024 affords, since those speeches tend to be long and contiguous. Let's make a new list-of-lists called `conv_sent_data`. Instead of each first entry in the sublists being an entire speech, make each first entry just a sentence from the speech. Feel free to use NLTK's `sent_tokenize` [function](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqiVB25Ds421"
      },
      "outputs": [],
      "source": [
        "conv_sent_data = []\n",
        "\n",
        "for speech, party in convention_data:\n",
        "    # Tokenize the speech into sentences\n",
        "    sentences = sent_tokenize(speech)\n",
        "    # Append each sentence with the associated party to conv_sent_data\n",
        "    for sentence in sentences:\n",
        "        conv_sent_data.append([sentence, party])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl9vuboGs421"
      },
      "source": [
        "Again, let's look at some random entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SrieJ6Us421",
        "outputId": "f62b0ff7-51a3-4827-fe36-47f7969f9ff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['chairman coming to you live from the wisconsin center it s time to begin our virtual trip around america our journey begins at the site of a major step forward in our national journey towards justice let s go to alabama',\n",
              "  'Democratic'],\n",
              " ['northern mariana islands', 'Democratic'],\n",
              " ['someone who looks like us on a presidential ticket that s crazy',\n",
              "  'Democratic'],\n",
              " ['he understands that leadership means fighting for the people who built this country all of you boyle allred kenyatta kimpson all of us',\n",
              "  'Democratic'],\n",
              " ['i m voting for joe biden because it s on my generation to make sure that we never go back',\n",
              "  'Democratic']]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "random.choices(conv_sent_data,k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czawaln9s421"
      },
      "source": [
        "Now it's time for our final cleaning before modeling. Go through `conv_sent_data` and take the following steps:\n",
        "\n",
        "1. Tokenize on whitespace\n",
        "1. Remove punctuation\n",
        "1. Remove tokens that fail the `isalpha` test\n",
        "1. Remove stopwords\n",
        "1. Casefold to lowercase\n",
        "1. Join the remaining tokens into a string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "clean_conv_sent_data = []  # list of tuples (cleaned sentence, party)\n",
        "\n",
        "for idx, sent_party in enumerate(conv_sent_data):\n",
        "    sentence, party = sent_party\n",
        "\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    # Remove punctuation, non-alphabetic tokens, and stopwords\n",
        "    cleaned_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "    # Join the remaining tokens into a cleaned sentence\n",
        "    cleaned_sentence = ' '.join(cleaned_tokens)\n",
        "\n",
        "    # Append the cleaned sentence and party to the clean_conv_sent_data list\n",
        "    clean_conv_sent_data.append((cleaned_sentence, party))\n",
        "\n",
        "random.choices(clean_conv_sent_data, k=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrTjQ9sj-qnS",
        "outputId": "b92d327f-bd51-4b52-baa6-a904b0c68795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('difficult times stand closest together tragedy go stronger', 'Democratic'),\n",
              " ('nebraska', 'Republican'),\n",
              " ('great back andrew thank please', 'Republican'),\n",
              " ('official roll call business republican convention conducted today charlotte created short video symbolize excitement president trump across states territories thank watching god bless god bless united states america',\n",
              "  'Republican'),\n",
              " ('thanks voters across country red states blue states fried kimpson nez r',\n",
              "  'Democratic')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icO2MUEOs422"
      },
      "source": [
        "If that looks good, let's make our function to turn these into features. First we need to build our list of candidate words. I started my exploration at a cutoff of 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNhm6vOgs422",
        "outputId": "22212d2a-96e1-4c0a-87ec-b1fadcc2b22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With a word cutoff of 5, we have 2376 as features in the model.\n"
          ]
        }
      ],
      "source": [
        "word_cutoff = 5\n",
        "\n",
        "tokens = [w for t, p in convention_data for w in t.split()]\n",
        "\n",
        "word_dist = nltk.FreqDist(tokens)\n",
        "\n",
        "feature_words = set()\n",
        "\n",
        "for word, count in word_dist.items() :\n",
        "    if count > word_cutoff :\n",
        "        feature_words.add(word)\n",
        "\n",
        "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PcP9t63s422"
      },
      "outputs": [],
      "source": [
        "def conv_features(text,fw) :\n",
        "    \"\"\"Given some text, this returns a dictionary holding the\n",
        "       feature words.\n",
        "\n",
        "       Args:\n",
        "            * text: a piece of text in a continuous string. Assumes\n",
        "            text has been cleaned and case folded.\n",
        "            * fw: the *feature words* that we're considering. A word\n",
        "            in `text` must be in fw in order to be returned. This\n",
        "            prevents us from considering very rarely occurring words.\n",
        "\n",
        "       Returns:\n",
        "            A dictionary with the words in `text` that appear in `fw`.\n",
        "            Words are only counted once.\n",
        "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
        "            then this would return a dictionary of\n",
        "            {'quick' : True,\n",
        "             'fox' :    True}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "    tokens = text.split()\n",
        "    ret_dict = dict()\n",
        "    for token in tokens:\n",
        "        if token in fw:\n",
        "            ret_dict[token] = True\n",
        "    return(ret_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "ORRXZ0jzs422",
        "outputId": "692eaecc-3415-4c52-a243-4188ed43f5c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-f391fe40f199>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m assert(conv_features(\"obama is the president\",feature_words)==\n\u001b[0m\u001b[1;32m      3\u001b[0m        {'obama':True,'president':True})\n\u001b[1;32m      4\u001b[0m assert(conv_features(\"some people in america are citizens\",feature_words)==\n\u001b[1;32m      5\u001b[0m                      {'people':True,'america':True,\"citizens\":True})\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "assert(len(feature_words)>0)\n",
        "assert(conv_features(\"obama is the president\",feature_words)==\n",
        "       {'obama':True,'president':True})\n",
        "assert(conv_features(\"some people in america are citizens\",feature_words)==\n",
        "                     {'people':True,'america':True,\"citizens\":True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf1yjIPDs422"
      },
      "source": [
        "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlfxDnZqs422"
      },
      "outputs": [],
      "source": [
        "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd82e7G-s422"
      },
      "outputs": [],
      "source": [
        "random.seed(20220507)\n",
        "random.shuffle(featuresets)\n",
        "\n",
        "test_size = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0bcpIr7Gs422",
        "outputId": "269346f6-b8f1-4a13-84fc-375c2019df8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.434\n"
          ]
        }
      ],
      "source": [
        "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RhVc3c8Ss422",
        "outputId": "f84314af-a2c3-4d10-fe89-821b1dba0cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                   china = True           Republ : Democr =     27.1 : 1.0\n",
            "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
            "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
            "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
            "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
            "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
            "                supports = True           Republ : Democr =     17.1 : 1.0\n",
            "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
            "                   media = True           Republ : Democr =     15.8 : 1.0\n",
            "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
            "               countries = True           Republ : Democr =     13.0 : 1.0\n",
            "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
            "                  defund = True           Republ : Democr =     13.0 : 1.0\n",
            "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
            "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
            "                religion = True           Republ : Democr =     13.0 : 1.0\n",
            "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
            "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
            "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
            "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
            "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
            "              department = True           Republ : Democr =     10.9 : 1.0\n",
            "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
            "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
            "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK1g5Albs422"
      },
      "source": [
        "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
        "\n",
        "### My Observations\n",
        "\n",
        "Based on the results of the classifier, my observations are as follows:\n",
        "\n",
        "Republican voters seem to be more focused on issues regarding: China, enforcement, destruction, freedoms, support, crime rates, media, beliefs, country pride, national defense, isis, liberals, religion, trade, flag, greatness, etc. The Republican voters are particularly keen on areas regarding trade with China, religion, ensuring personal freedoms, and upholding conservative values. Foreign policy with China seems to be their most pressing issue according to tweets while the amendments are their least pressing issue.\n",
        "\n",
        "Democrat voters seem to be more focused on issues such as voting rights/voting registration and climate change with voting rights and registration being more of a pressing issue than climate change.\n",
        "\n",
        "There is not much overlap in terms of rhetoric, with Republican voters' rhetoric being more on the extreme side. Republican voters also have a bias against mainstream media as 'media' is one of the more pressing issues Republicans feel they are facing. Their tweets tend to indicate disdain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvh5KJ9Ns422"
      },
      "source": [
        "## Part 2: Classifying Congressional Tweets\n",
        "\n",
        "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
        "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
        "give you the query I used to pull out the tweets. Note that this DB has some big tables and\n",
        "is unindexed, so the query takes a minute or two to run on my machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHwkkzivs422"
      },
      "outputs": [],
      "source": [
        "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
        "cong_cur = cong_db.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qd5oOic8s422",
        "outputId": "989cd67a-4b8f-454a-b11a-4d7156cc61d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Mo Brooks',\n",
              "  'Republican',\n",
              "  b'\"Brooks Joins Alabama Delegation in Voting Against Flawed Funding Bill\" http://t.co/3CwjIWYsNq')]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "results = cong_cur.execute(\n",
        "        '''\n",
        "           SELECT DISTINCT\n",
        "                  cd.candidate,\n",
        "                  cd.party,\n",
        "                  tw.tweet_text\n",
        "           FROM candidate_data cd\n",
        "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle\n",
        "               AND cd.candidate == tw.candidate\n",
        "               AND cd.district == tw.district\n",
        "           WHERE cd.party in ('Republican','Democratic')\n",
        "               AND tw.tweet_text NOT LIKE '%RT%'\n",
        "        ''')\n",
        "\n",
        "results = list(results) # Just to store it, since the query is time consuming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WpUfpdeOs422",
        "outputId": "538b29c1-4b87-4db7-dcaa-d7005ff46e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['brooks joins alabama delegation in voting against flawed funding bill http', 'Republican']]\n"
          ]
        }
      ],
      "source": [
        "tweet_data = []\n",
        "\n",
        "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
        "# Note that this may take a bit of time, since we have a lot of tweets.\n",
        "\n",
        "for row in results:\n",
        "    party = row[1]\n",
        "    name = row[0]\n",
        "    tweet = row[2].decode('utf-8')  # Convert the tweet text to a string\n",
        "    cleaned_text = clean_tokenize(tweet)\n",
        "    tweet_data.append([cleaned_text, party])\n",
        "\n",
        "print(tweet_data[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_XFOAjBs422"
      },
      "source": [
        "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIo8rO3Xs422"
      },
      "outputs": [],
      "source": [
        "random.seed(20201014)\n",
        "\n",
        "tweet_data_sample = random.choices(tweet_data,k=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tweet_features(words):\n",
        "    return {word: True for word in words}\n",
        "\n",
        "# Create feature sets using the extract_tweet_features function\n",
        "featuresets = [(extract_tweet_features(tweet), party) for tweet, party in tweet_data]\n",
        "\n",
        "random.seed(20201014)\n",
        "random.shuffle(featuresets)\n",
        "\n",
        "split_index = int(len(featuresets) * 0.7)\n",
        "train_set = featuresets[:split_index]\n",
        "test_set = featuresets[split_index:]\n",
        "\n",
        "# Train the Naive Bayes classifier using the training set\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Classify the sample tweets and compare the results to the actual party labels\n",
        "for tweet, party in tweet_data_sample:\n",
        "    tweet_features = extract_tweet_features(tweet)\n",
        "    estimated_party = classifier.classify(tweet_features)\n",
        "\n",
        "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
        "    print(f\"Actual party is {party} and our classifier says {estimated_party}.\")\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hEPe-lo6Fh1Z",
        "outputId": "bf465224-a09d-4706-8445-6bec01adf79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's our (cleaned) tweet: the mass shooting in las vegas was a horrific act of violence the victims and their families are in my thoughts and prayers\n",
            "Actual party is Democratic and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: early morning this traveltuesday leaving for dc http\n",
            "Actual party is Republican and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: the only moderates in iraq amp syria are civilians we enemies on both sides of the conflict we should not assist either\n",
            "Actual party is Republican and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: rt natsecaction over national security veterans are demanding answers on the release of the confidential national security\n",
            "Actual party is Democratic and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: buildthatwall now https\n",
            "Actual party is Republican and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: i was glad to attend the and assure everyone i could that majority of americans still stand by our traditional allies\n",
            "Actual party is Democratic and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: cnn everyone wraps themselves in the flag and patriotism to avoid the discussion about racism and injustice kneeling is honoring our troops\n",
            "Actual party is Democratic and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: i applaud president trump decision to send the national guard to protect our border congress should support the president on this including fully funding the wall it time to stop playing politics with the national security of the united states fundthewall nationalguard\n",
            "Actual party is Republican and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: as congress considers disaster relief spending this year we must include funding for california fire relief listen to my remarks on the house floor here https\n",
            "Actual party is Democratic and our classifier says Democratic.\n",
            "\n",
            "Here's our (cleaned) tweet: proud to support oss helped vanquish some of most malevolent enemies our and free has ever faced https\n",
            "Actual party is Democratic and our classifier says Democratic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6UchrwMs423"
      },
      "source": [
        "Now that we've looked at it some, let's score a bunch and see how we're doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMV1MhCws423"
      },
      "outputs": [],
      "source": [
        "# dictionary of counts by actual party and estimated party.\n",
        "# first key is actual, second is estimated\n",
        "parties = ['Republican','Democratic']\n",
        "results = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for p in parties :\n",
        "    for p1 in parties :\n",
        "        results[p][p1] = 0\n",
        "\n",
        "num_to_score = 10000\n",
        "random.shuffle(tweet_data)\n",
        "\n",
        "for idx, tp in enumerate(tweet_data) :\n",
        "    tweet, party = tp\n",
        "    # Now do the same thing as above, but we store the results rather\n",
        "    # than printing them.\n",
        "\n",
        "    # get the estimated party\n",
        "    featuresets = extract_tweet_features(tweet)\n",
        "    estimated_party = classifier.classify(featuresets)\n",
        "\n",
        "    results[party][estimated_party] += 1\n",
        "\n",
        "    if idx > num_to_score :\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ecNYEFZJs423",
        "outputId": "697a7e90-1568-4434-fd1d-344dc54681d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>()>,\n",
              "            {'Republican': defaultdict(int,\n",
              "                         {'Republican': 1, 'Democratic': 4072}),\n",
              "             'Democratic': defaultdict(int,\n",
              "                         {'Republican': 0, 'Democratic': 5929})})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSHGfKAWs423"
      },
      "source": [
        "### Reflections\n",
        "\n",
        "Based on these results, there is 1 Republican tweet correctly identified as Republican with 4072 incorrectly identified as Democrat when it was a Republican tweet.\n",
        "\n",
        "5929 Democrat tweets were correctly identified as Democrat tweets while 0 Democrat tweets were identified as Republican. This is not very likely and indicates that the code did not work correctly somewhere along the process."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}